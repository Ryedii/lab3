好的！根据你提供的 **exp3.pdf** 文件内容，这份实验手册的目的是指导学生在 **昇腾（Ascend）硬件平台**（具体为香橙派 AI Pro 20T开发板，搭载昇腾310B NPU）上完成一个 **大语言模型（LLM）的端侧部署**。

整个实验流程可以概括为 **“准备 -> 量化 -> 转换 -> 编译 -> 推理”** 五个核心阶段。以下是详细的实验流程介绍：

---

### **一、 实验目标与背景**

*   **目标**：掌握将开源大语言模型（TinyLlama）部署到资源受限的端侧设备（香橙派）上的完整流程。
*   **关键技术**：模型量化（W8A8）、ONNX模型格式、昇腾工具链（ATC编译器）、NPU推理。
*   **硬件平台**：香橙派 AI Pro 20T（集成昇腾310B NPU）。
*   **基础模型**：`TinyLlama-chat-v1.0`，一个轻量化的Llama变体。

---

### **二、 核心实验流程**

#### **步骤 1: 准备环境 (Environment Setup)**
这是实验的基础，确保开发板和软件环境就绪。
*   **连接开发板**：手册提供了多种连接方式（显示器直连、网线直连、USB串口连接），推荐使用显示器或USB串口。
*   **安装依赖**：
    *   安装 `protobuf`（用于ONNX等序列化）。
    *   配置昇腾软件栈（CANN）环境变量。
    *   **编译自定义算子**：由于量化后的模型可能包含标准算子库不支持的操作（如`MatMulInteger`），需要手动编译并安装一个自定义算子插件。

#### **步骤 2: 量化模型 (Model Quantization)**
为了适应端侧设备有限的算力和内存，必须对原始FP16/FP32模型进行压缩。
*   **量化方案**：采用 **W8A8**（权重8-bit整数，激活值8-bit整数）的**训练后量化**（PTQ）。
*   **关键操作**：
    1.  **收集激活值范围**：使用少量校准数据（如wikitext）运行模型，并通过 **PyTorch Hook** 机制统计每个线性层输入激活值的最大绝对值。
    2.  **执行量化**：根据收集到的权重和激活值范围，计算缩放因子（scale）和零点（zero point），并将模型中的 `nn.Linear` 层替换为自定义的 `W8X8Linear` 量化层。

#### **步骤 3: 导出 ONNX 模型 (Export to ONNX)**
ONNX 是一个通用的模型中间表示，是连接训练框架（PyTorch）和昇腾硬件的关键桥梁。
*   **加载模型**：加载上一步量化好的 TinyLlama 模型。
*   **构造示例输入**（Dummy Input）：为 `torch.onnx.export` 函数提供符合模型输入签名的张量，包括：
    *   `input_ids`: 输入的token ID序列。
    *   `attention_mask`: 注意力掩码。
    *   `position_ids`: 位置ID。
    *   `past_key_values`: 用于缓存历史KV以加速自回归生成的张量（形状需特别注意）。
*   **定义动态轴**（Dynamic Axes）：声明哪些维度（如batch size, sequence length）是可变的，使导出的ONNX模型能处理不同长度的输入。
*   **执行导出**：调用 `torch.onnx.export` 生成 `.onnx` 文件。

#### **步骤 4: 使用 ATC 编译生成 OM 模型 (Compile with ATC)**
昇腾的 **Ascend Tensor Compiler **(ATC) 工具负责将ONNX模型转换为昇腾NPU可以直接高效执行的离线模型（OM模型）。
*   **执行命令**：在终端运行 `atc` 命令，指定输入ONNX文件、输出路径、输入张量的具体形状、目标芯片（`Ascend310B`）等参数。
*   **输出**：生成一个 `.om` 文件（例如 `tinyllama-chat-v1.0.om`），这是最终部署到NPU上的模型文件。

#### **步骤 5: 推理与输出生成 (Inference and Generation)**
最后一步是在开发板上编写Python脚本，加载OM模型并实现完整的对话生成功能。
*   **实现聊天模板**（`apply_chat_template`）：将用户和AI的历史对话（包含system、user、assistant角色）按照特定格式（如`<|user|>...<|assistant|>`）拼接成模型能理解的prompt。
*   **实现采样策略**（`sample_logits_top_k`）：模型每一步会输出一个 `logits` 向量（代表词表中每个词的概率）。需要实现 **Top-k采样** 等策略，从这个向量中选择下一个最可能的token。
*   **构建推理主循环**（`predict`）：
    1.  将用户输入格式化为prompt。
    2.  Tokenize后送入OM模型。
    3.  获取模型返回的 `logits`。
    4.  使用采样策略得到下一个token ID。
    5.  将新token追加到输出序列，并更新 `past_key_values` 和 `attention_mask`。
    6.  循环执行3-5步，直到生成结束符（EOS）或达到最大长度。
    7.  将token ID序列解码回人类可读的文本并输出。

---

### **三、 总结**

整个实验流程是一个典型的 **“云端训练 -> 端侧部署”** 的闭环：
1.  **优化**：通过量化技术压缩模型。
2.  **标准化**：通过ONNX实现框架无关性。
3.  **硬件适配**：通过ATC编译器将通用模型转换为昇腾NPU专属的高效执行格式。
4.  **应用**：编写推理引擎，将模型能力封装成可用的聊天功能。

这个过程让学生深入理解了从模型算法到硬件落地的每一个关键环节。